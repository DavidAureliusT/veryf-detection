{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "734089d7",
   "metadata": {},
   "source": [
    "# Detect Invoice Number From Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22c23722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging (1)\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "import time\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "tf.get_logger().setLevel('ERROR')           # Suppress TensorFlow logging (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "287520a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7da5192",
   "metadata": {},
   "source": [
    "###  Variables:\n",
    "       - IMAGE_PATH : String       -> .jpg || .jpeg file\n",
    "       - LABEL_MAP_PATH : String   -> .pbtxt file\n",
    "       - SAVED_MODEL_PATH : String -> \"reference to saved_model folder of exported model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c90b609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH          = \"./images/sample2.jpg\"\n",
    "LABEL_MAP_PATH      = \"./data/label_map.pbtxt\"\n",
    "SAVED_MODEL_PATH    = \"./data/saved_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c17da0e",
   "metadata": {},
   "source": [
    "### 1. Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "02347817",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_fn = tf.saved_model.load(SAVED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2db123",
   "metadata": {},
   "source": [
    "### 2. Load label map data (for plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ebe2fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(LABEL_MAP_PATH, use_display_name=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45e902e",
   "metadata": {},
   "source": [
    "### 3. Convert image into numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cef8ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "    return np.array(Image.open(path))\n",
    "\n",
    "image_np = load_image_into_numpy_array(IMAGE_PATH) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff6edb2",
   "metadata": {},
   "source": [
    "### 4. The input needs to be a tensor, convert it using `tf.convert_to_tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a33e0941",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = tf.convert_to_tensor(image_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb780e6",
   "metadata": {},
   "source": [
    "### 5. The model expects a batch of images, so add an axis with `tf.newaxis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "54cddd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = input_tensor[tf.newaxis, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5647b8",
   "metadata": {},
   "source": [
    "### 6. Run Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "04f5c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = detect_fn(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46b782e",
   "metadata": {},
   "source": [
    "### 7. All outputs are batches tensors. \n",
    "**Convert to numpy arrays, and take index [0] to remove the batch dimension. We're only interested in the first num_detections.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "961eac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_detections = int(detections.pop('num_detections'))\n",
    "detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "detections['num_detections'] = num_detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b2bcb",
   "metadata": {},
   "source": [
    "### 8. detection_classes should be ints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "21474c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "image_np_with_detections = image_np.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6bdf64",
   "metadata": {},
   "source": [
    "### *Detection Result Breakdown*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd35b05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04715748, 0.15597913, 0.24276222, 0.24058676], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detections['detection_boxes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f17db51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0], dtype=int64),)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(detections['detection_scores'] == detections['detection_scores'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "076392c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048, 1536, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_np_with_detections.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a3d72e",
   "metadata": {},
   "source": [
    "# Draw Bounding Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5ffc630a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7322491407394409, 0.8059602379798889, 0.9526413679122925, 0.8902712464332581)\n"
     ]
    }
   ],
   "source": [
    "box = tuple(detections['detection_boxes'][0].tolist())\n",
    "print(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "83a050da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoice-number\n"
     ]
    }
   ],
   "source": [
    "classes = detections['detection_classes']\n",
    "class_name = category_index[classes[0]]['name']\n",
    "print(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "11bd593e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59%\n"
     ]
    }
   ],
   "source": [
    "score = '{}%'.format(round(100*detections['detection_scores'][0]))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f95657d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ymin, xmin, ymax, xmax = box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "943a2d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pil = Image.fromarray(np.uint8(image_np_with_detections)).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d3281473",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw = ImageDraw.Draw(image_pil)\n",
    "im_width, im_height = image_pil.size\n",
    "# using normalize_coordinate\n",
    "(left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
    "                                  ymin * im_height, ymax * im_height)\n",
    "# draw line\n",
    "draw.line([(left, top), (left, bottom), (right, bottom), (right, top),\n",
    "               (left, top)],\n",
    "              width=4,\n",
    "              fill='red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5eb8f3",
   "metadata": {},
   "source": [
    "# Croping Invoice Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "036fa960",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_crop = image_pil.crop((left, top, right, bottom))\n",
    "im_crop.show()\n",
    "im_crop.save(\"./images/cropped_img2.jpeg\", \"JPEG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "070243f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536, 2048)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pil.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa7fe8a",
   "metadata": {},
   "source": [
    "# Extract Invoice Number using Tesseract\n",
    "*Preprocessing :*\n",
    "1. Resize image 4x\n",
    "2. Clear the noise\n",
    "3. Sharpen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a35dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f315214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename\n",
    "images_path = \"images/\"\n",
    "img_cropped_filename = \"cropped_img.jpeg\"                      # 96 DPI\n",
    "\n",
    "img_resized_filename = \"cropped_img_resized.png\"               # 300 DPI\n",
    "img_cropped_edited_filename = \"cropped_img_edited.jpeg\"        # 96 DPI\n",
    "img_resized_edited_filename = \"cropped_img_resized_edited.png\" # 300 DPI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a5c33f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(filename):\n",
    "    low_res_img_pil = Image.open(images_path + filename)\n",
    "    h, w = low_res_img_pil.size\n",
    "    resize = h*4, w*4\n",
    "    im_resized = low_res_img_pil.resize(resize, Image.ANTIALIAS)\n",
    "    resized_img_filename =  filename.split(\".\")[0] + \"_resized.jpeg\"\n",
    "    im_resized.save(images_path + resized_img_filename, \"JPEG\")\n",
    "    return resized_img_filename\n",
    "\n",
    "def denoise_image(filename):\n",
    "    noised_img_cv = cv2.imread(images_path + filename)\n",
    "    denoised_img = cv2.fastNlMeansDenoising(noised_img_cv, None, 10)\n",
    "    denoised_img_filename =  filename.split(\".\")[0] + \"_denoised.jpeg\"\n",
    "    cv2.imwrite(images_path + denoised_img_filename, denoised_img)\n",
    "    return denoised_img_filename\n",
    "    \n",
    "def sharpen_image(filename):\n",
    "    unsharpen_img_cv = cv2.imread(images_path + filename)\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1, 5,-1],\n",
    "                       [0, -1, 0]])\n",
    "    sharpened_img = cv2.filter2D(src=unsharpen_img_cv, ddepth=-1, kernel=kernel)\n",
    "    sharpened_img_filename =  filename.split(\".\")[0] + \"_sharpened.jpeg\"\n",
    "    cv2.imwrite(images_path + sharpened_img_filename, sharpened_img)\n",
    "    return sharpened_img_filename\n",
    "\n",
    "def scale_contrast_image(filename):\n",
    "    image = cv2.imread(images_path + filename)\n",
    "    new_image = np.zeros(image.shape, image.dtype)\n",
    "    alpha = 2.0 # Simple contrast control\n",
    "    beta = 1    # Simple brightness control\n",
    "    \n",
    "    for y in range(image.shape[0]):\n",
    "        for x in range(image.shape[1]):\n",
    "            for c in range(image.shape[2]):\n",
    "                new_image[y,x,c] = np.clip(alpha*image[y,x,c] + beta, 0, 255)\n",
    "    new_filename = filename.split(\".\")[0] + \"_contrast.jpeg\"\n",
    "    cv2.imwrite(images_path + new_filename, new_image)\n",
    "    return new_filename\n",
    "\n",
    "def adaptive_thresholding_image(filename):\n",
    "    image = cv2.imread(images_path + filename)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#     ret,thresh1 = cv2.threshold(image,200,255,cv2.THRESH_BINARY)\n",
    "    new_image = cv2.adaptiveThreshold(image,255,cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY,31,3)\n",
    "    new_filename = filename.split(\".\")[0] + \"_at.jpeg\"\n",
    "    cv2.imwrite(images_path + new_filename, new_image)\n",
    "    return new_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "19207fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Image Preprocessing Pipepline\n",
    "# image_choice = \"sample2_cropped.jpeg\"\n",
    "# resized_image = resize_image(image_choice)\n",
    "# sharpened_image = sharpen_image(resized_image)\n",
    "# denoised_image = denoise_image(sharpened_image)\n",
    "# denoised_image_2x = denoise_image(denoised_image)\n",
    "# contrast_image = scale_contrast_image(denoised_image_2x)\n",
    "at_image = adaptive_thresholding_image(contrast_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805defdc",
   "metadata": {},
   "source": [
    "# Good to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "baf9872c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./images/sample2_cropped_resized_sharpened_denoised_2x_contrast_at.jpeg shape: (516, 1804)\n",
      "./images/sample2_cropped_resized_sharpened_denoised_2x_contrast_at.jpeg max val: <built-in method max of numpy.ndarray object at 0x0000022CE352DA50>\n",
      "86269\n",
      "\n",
      "- -\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read Image (opencv)\n",
    "images_path = \"./images/\"\n",
    "# goal = images_path + img_resized_edited_filename\n",
    "# test = images_path + bt_image\n",
    "asd = images_path + \"sample2_cropped_resized_sharpened_denoised_2x_contrast_at.jpeg\"\n",
    "choosedImage = asd\n",
    "# choosedImage = img_resized_edited_filename\n",
    "img_cv = cv2.imread(choosedImage)\n",
    "img_cv = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)\n",
    "img_cv = cv2.rotate(img_cv, cv2.ROTATE_90_CLOCKWISE)\n",
    "# ret,img_cv = cv2.threshold(img_cv,100,255,cv2.THRESH_BINARY)\n",
    "img_cv = cv2.adaptiveThreshold(img_cv,255,cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY,31,3)\n",
    "print(choosedImage + \" shape: \" + str(img_cv.shape))\n",
    "print(choosedImage + \" max val: \" + str(img_cv.max))\n",
    "cv2.imshow(choosedImage,img_cv)\n",
    "cv2.waitKey(0)\n",
    "print(pytesseract.image_to_string(img_cv, config=\"outputbase digits\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "37b83699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: (5) 69798 \n",
      "count: (5) 86269 \n",
      "count: (5) 86269 \n",
      "count: (5) 69798 \n",
      "{'data': ['69798', '86269', '86269', '69798']}\n"
     ]
    }
   ],
   "source": [
    "ocr_result = []\n",
    "for x in range(4):\n",
    "    img_cv = cv2.rotate(img_cv, cv2.ROTATE_90_CLOCKWISE)\n",
    "    _results = pytesseract.image_to_string(img_cv, config=\"outputbase digits\")\n",
    "    _results = _results.split('\\n')\n",
    "    for _result in _results:\n",
    "        if len(_result) == 5:\n",
    "            ocr_result.append(_result)\n",
    "for res in ocr_result:\n",
    "    print(\"count: (\"+str(len(res))+\") \" + res + \" \")\n",
    "\n",
    "response = {\n",
    "    \"data\" : ocr_result\n",
    "}\n",
    "    \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d537593f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\": [\"69798\", \"86269\", \"86269\", \"69798\"]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "982fc69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "a = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fe2628a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 12.768976926803589 seconds ---\n"
     ]
    }
   ],
   "source": [
    "b = time.time()\n",
    "print(\"--- %s seconds ---\" % (b-a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
