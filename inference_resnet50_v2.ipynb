{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eb1f7fb",
   "metadata": {},
   "source": [
    "# Detect Invoice Number From Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22c23722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging (1)\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "import time\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "tf.get_logger().setLevel('ERROR')           # Suppress TensorFlow logging (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "287520a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7da5192",
   "metadata": {},
   "source": [
    "###  Variables:\n",
    "       - IMAGE_PATH : String       -> .jpg || .jpeg file\n",
    "       - LABEL_MAP_PATH : String   -> .pbtxt file\n",
    "       - SAVED_MODEL_PATH : String -> \"reference to saved_model folder of exported model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c90b609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH          = \"./sample.jpg\"\n",
    "LABEL_MAP_PATH      = \"./label_map.pbtxt\"\n",
    "SAVED_MODEL_PATH    = \"./saved_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c17da0e",
   "metadata": {},
   "source": [
    "### 1. Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02347817",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_fn = tf.saved_model.load(SAVED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2db123",
   "metadata": {},
   "source": [
    "### 2. Load label map data (for plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebe2fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(LABEL_MAP_PATH, use_display_name=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45e902e",
   "metadata": {},
   "source": [
    "### 3. Convert image into numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cef8ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "    return np.array(Image.open(path))\n",
    "\n",
    "image_np = load_image_into_numpy_array(IMAGE_PATH) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff6edb2",
   "metadata": {},
   "source": [
    "### 4. The input needs to be a tensor, convert it using `tf.convert_to_tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a33e0941",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = tf.convert_to_tensor(image_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb780e6",
   "metadata": {},
   "source": [
    "### 5. The model expects a batch of images, so add an axis with `tf.newaxis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54cddd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = input_tensor[tf.newaxis, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5647b8",
   "metadata": {},
   "source": [
    "### 6. Run Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04f5c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = detect_fn(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46b782e",
   "metadata": {},
   "source": [
    "### 7. All outputs are batches tensors. \n",
    "**Convert to numpy arrays, and take index [0] to remove the batch dimension. We're only interested in the first num_detections.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "961eac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_detections = int(detections.pop('num_detections'))\n",
    "detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "detections['num_detections'] = num_detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b2bcb",
   "metadata": {},
   "source": [
    "### 8. detection_classes should be ints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21474c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "image_np_with_detections = image_np.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6bdf64",
   "metadata": {},
   "source": [
    "### *Detection Result Breakdown*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd35b05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04715748, 0.15597913, 0.24276222, 0.24058676], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detections['detection_boxes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f17db51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0], dtype=int64),)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(detections['detection_scores'] == detections['detection_scores'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "076392c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048, 1536, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_np_with_detections.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a3d72e",
   "metadata": {},
   "source": [
    "# Draw Bounding Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ffc630a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.04715748131275177, 0.15597912669181824, 0.2427622228860855, 0.2405867576599121)\n"
     ]
    }
   ],
   "source": [
    "box = tuple(detections['detection_boxes'][0].tolist())\n",
    "print(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83a050da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoice-number\n"
     ]
    }
   ],
   "source": [
    "classes = detections['detection_classes']\n",
    "class_name = category_index[classes[0]]['name']\n",
    "print(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11bd593e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83%\n"
     ]
    }
   ],
   "source": [
    "score = '{}%'.format(round(100*detections['detection_scores'][0]))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f95657d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ymin, xmin, ymax, xmax = box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "943a2d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pil = Image.fromarray(np.uint8(image_np_with_detections)).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d3281473",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw = ImageDraw.Draw(image_pil)\n",
    "im_width, im_height = image_pil.size\n",
    "# using normalize_coordinate\n",
    "(left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
    "                                  ymin * im_height, ymax * im_height)\n",
    "# draw line\n",
    "draw.line([(left, top), (left, bottom), (right, bottom), (right, top),\n",
    "               (left, top)],\n",
    "              width=4,\n",
    "              fill='red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5eb8f3",
   "metadata": {},
   "source": [
    "# Croping Invoice Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "036fa960",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_crop = image_pil.crop((left, top, right, bottom))\n",
    "im_crop.show()\n",
    "im_crop.save(\"cropped_img\", \"JPEG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa7fe8a",
   "metadata": {},
   "source": [
    "# Extract Invoice Number using Tesseract\n",
    "*Preprocessing :*\n",
    "1. Resize image 4x\n",
    "2. Clear the noise\n",
    "3. Sharpen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a35dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f315214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename\n",
    "img_cropped_filename = \"cropped_img.jpeg\"                      # 96 DPI\n",
    "img_resized_filename = \"cropped_img_resized.png\"               # 300 DPI\n",
    "img_cropped_edited_filename = \"cropped_img_edited.jpeg\"        # 96 DPI\n",
    "img_resized_edited_filename = \"cropped_img_resized_edited.png\" # 300 DPI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c33f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(path):\n",
    "    low_res_img_pil = Image.open(path)\n",
    "    h, w = low_res_img_pil.size\n",
    "    resize = h*4, w*4\n",
    "    im_resized = low_res_img_pil.resize(resize, Image.ANTIALIAS)\n",
    "    im_resized.save(\"cropped_img_resized.png\", \"PNG\")\n",
    "\n",
    "def remove_noise(path):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b436140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_res_img_pil = Image.open(\"cropped_img.jpeg\")\n",
    "low_res_img_pil.size\n",
    "h, w = low_res_img_pil.size\n",
    "resize = h*4, w*4\n",
    "im_resized = low_res_img_pil.resize(resize, Image.ANTIALIAS)\n",
    "im_resized.save(\"cropped_img_resized.png\", \"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "baf9872c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cropped_img_resized_edited.png shape: (520, 1600, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Image (opencv)\n",
    "choosedImage = img_resized_edited_filename\n",
    "img_cv = cv2.imread(choosedImage)\n",
    "img_cv = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)\n",
    "print(choosedImage + \" shape: \" + str(img_cv.shape))\n",
    "cv2.imshow(choosedImage,img_cv)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c2ab47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69798\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pytesseract.image_to_string(img_cv, config=\"outputbase digits\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cbc99716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpi: (96, 96)\n"
     ]
    }
   ],
   "source": [
    "# Up-Scale the image resolution\n",
    "# low_res_img_pil = Image.open(\"cropped_img.jpeg\")\n",
    "# print(\"dpi: \" + str(low_res_img_pil.info['dpi']))\n",
    "# low_res_img_pil.save(\"cropped_img-600.png\", dpi=(600,600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "92a72618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_res_img_pil.size\n",
    "# h, w = low_res_img_pil.size\n",
    "# resize = h*4, w*4\n",
    "# im_resized = low_res_img_pil.resize(resize, Image.ANTIALIAS)\n",
    "# im_resized.save(\"cropped_img_resized.png\", \"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a57bd868",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "shape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10976/1424130809.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mhi_res_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cropped_img_resized.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"new dpi: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhi_res_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\deploy-veryf-detection\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    517\u001b[0m             )\n\u001b[0;32m    518\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_category\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: shape"
     ]
    }
   ],
   "source": [
    "# hi_res_image = Image.open(\"cropped_img_resized.png\")\n",
    "# print(\"new dpi: \" + str(hi_res_image.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "15b6968e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130, 400, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upscale_image = cv2.imread('cropped_img-600.png')\n",
    "print(upscale_image.shape)\n",
    "cv2.imshow('image',upscale_image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "im = Image.open(\"my_image.png\")\n",
    "size = 7016, 4961\n",
    "im_resized = im.resize(size, Image.ANTIALIAS)\n",
    "im_resized.save(\"my_image_resized.png\", \"PNG\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
